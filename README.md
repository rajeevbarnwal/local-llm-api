# local-llm-api

A step-by-step project to run a local LLM API with Python, Ollama, and FastAPI.

## ðŸ“‹ Overview

This repository shows you how to:

- Install and run open-source LLMs locally via Ollama.
- Build a lightweight REST API in Python using FastAPI.
- Send prompts to your local model and receive responses without external services.

## ðŸ”§ Prerequisites

- Python 3.9+ installed
- Docker or native installation of Ollama (https://ollama.com/)
- Basic familiarity with Python and REST APIs

## ðŸš€ Installation

1. Clone this repo:
   ```bash
   git clone https://github.com/rajeevbarnwal/local-llm-api.git
   cd local-llm-api